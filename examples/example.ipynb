{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12ad970",
   "metadata": {},
   "source": [
    "## `wslearn` example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66483530",
   "metadata": {},
   "source": [
    "In this example, we use `wslearn` to train a ResNet model on the CIFAR10 dataset. `wslearn` is designed to make weakly supervised learning workflows look similar to conventional supervised learning. A `wslearn` script looks very similar to typical Torch style code, with a model, dataset, dataloader, optimizer, and training loop. There are some differences however which we will discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c507b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wslearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38474b33",
   "metadata": {},
   "source": [
    "`wslearn` provides ready-made datasets for use. Examples from `wslearn` datasets have transformations included for use with consistency-regularisation algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f10449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wslearn.datasets import Cifar10\n",
    "\n",
    "data = Cifar10(lbls_per_class=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1af47",
   "metadata": {},
   "source": [
    "There are separate labelled and unlabelled datasets. When accessing examples, the output is a dictionary of the original data, it's label and the transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c779d92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['y', 'weak', 'strong', 'X'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_lbl_dataset()[1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd46d2",
   "metadata": {},
   "source": [
    "The unlabelled observations do not have labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc6a1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['weak', 'strong', 'X'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_ulbl_dataset()[1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8dd61d",
   "metadata": {},
   "source": [
    "`wslearn` provides an implementation of FixMatch (https://arxiv.org/pdf/2001.07685). There are several parameters we can customise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "586b62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wslearn.algorithms import FixMatch\n",
    "\n",
    "algorithm = FixMatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9138f",
   "metadata": {},
   "source": [
    "We will use a pretrained Vision transformer and fine tune it for our application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc26030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhamid/miniconda3/envs/wslearn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from wslearn.networks.vision_transfomers import ViT_Tiny_2\n",
    "\n",
    "model = ViT_Tiny_2(32, 3, 10)\n",
    "model.load_checkpoint(\"https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth\")\n",
    "\n",
    "\n",
    "# model = vit_tiny_patch2_32(pretrained=True, pretrained_path=\"https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c1f19",
   "metadata": {},
   "source": [
    "`wslearn` provides specialised dataloaders for handling labelled and unlabelled batches. The CyclicLoader will reshuffle the labelled and unlabelled data separately once they have been consumed. This means the dataloader will never terminate. Output from the CyclicLoader is a tuple labelled_batch, unlabelled_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a755d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wslearn.utils.data import CyclicLoader\n",
    "\n",
    "lbl_batch_size = 8\n",
    "ulbl_batch_size = 16\n",
    "train_loader = CyclicLoader(data.get_lbl_dataset(), data.get_ulbl_dataset(),\n",
    "                               lbl_batch_size=lbl_batch_size, ulbl_batch_size=ulbl_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bdbed",
   "metadata": {},
   "source": [
    "We can simply use Adam as provided by Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e2184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78756e0",
   "metadata": {},
   "source": [
    "We now need to write a training function. In a wsl context, we prefer to use training iterations rather than epochs, as the idea of an epoch makes less sense with two datasets in parallel. This training loop is otherwise very conventional with the exception of the main training logic being handed over to `algorithm.forward()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939fdc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_device(d, device):\n",
    "    return {k: v.to(device) if torch.is_tensor(v) else v for k, v in d.items()}\n",
    "\n",
    "def train(model, train_loader, algorithm,  optimizer, num_iters=128,\n",
    "          num_log_iters = 8, device=\"cpu\"):\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    training_bar = tqdm(train_loader, total=num_iters, desc=\"Training\",\n",
    "                        leave=True)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i, (lbl_batch, ulbl_batch) in enumerate(training_bar):\n",
    "\n",
    "        lbl_batch = dict_to_device(lbl_batch, device)\n",
    "        ulbl_batch = dict_to_device(ulbl_batch, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = algorithm.forward(model, lbl_batch, ulbl_batch)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (i+1)\n",
    "\n",
    "        if i % num_log_iters == 0:\n",
    "            training_bar.set_postfix(avg_loss = round(avg_loss, 4))\n",
    "\n",
    "        if i > num_iters:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb956384",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3addcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 129it [00:15,  8.49it/s, avg_loss=0.919]                         \n"
     ]
    }
   ],
   "source": [
    "train(model=model, train_loader=train_loader, algorithm=algorithm,\n",
    "      optimizer=optimizer, device=device, num_iters=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9979e72",
   "metadata": {},
   "source": [
    "Now the model has finished training, we can evaluate it's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b9e0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix\n",
    ")\n",
    "\n",
    "def evaluate(model, eval_loader, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_num = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "            num_batch = y.shape[0]\n",
    "            total_num += num_batch\n",
    "            logits = model(X)\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(torch.max(logits, dim=-1)[1].cpu().tolist())\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        print(\"accuracy: \", acc)\n",
    "        cf_mat = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "        print('confusion matrix:\\n' + np.array_str(cf_mat))\n",
    "        model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9162594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6558\n",
      "confusion matrix:\n",
      "[[0.747 0.012 0.    0.001 0.006 0.004 0.002 0.074 0.144 0.01 ]\n",
      " [0.    0.916 0.    0.    0.001 0.    0.    0.005 0.026 0.052]\n",
      " [0.104 0.007 0.507 0.029 0.15  0.098 0.03  0.061 0.014 0.   ]\n",
      " [0.031 0.021 0.015 0.413 0.184 0.21  0.042 0.055 0.027 0.002]\n",
      " [0.052 0.002 0.005 0.069 0.624 0.038 0.027 0.17  0.012 0.001]\n",
      " [0.016 0.004 0.001 0.032 0.09  0.806 0.001 0.042 0.008 0.   ]\n",
      " [0.003 0.006 0.008 0.044 0.08  0.018 0.828 0.008 0.005 0.   ]\n",
      " [0.106 0.006 0.003 0.028 0.308 0.112 0.007 0.386 0.041 0.003]\n",
      " [0.101 0.069 0.    0.    0.003 0.001 0.001 0.032 0.748 0.045]\n",
      " [0.048 0.241 0.    0.    0.005 0.001 0.    0.034 0.088 0.583]]\n"
     ]
    }
   ],
   "source": [
    "eval_loader = DataLoader(data.get_eval_dataset(), batch_size=32)\n",
    "evaluate(model, eval_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wslearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
