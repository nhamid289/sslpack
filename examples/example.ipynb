{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12ad970",
   "metadata": {},
   "source": [
    "## `wslearn` basic example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66483530",
   "metadata": {},
   "source": [
    "In this example, we use `wslearn` to train a ResNet model on the CIFAR10 dataset. `wslearn` is designed to make weakly supervised learning workflows look similar to conventional supervised learning. A `wslearn` script looks very similar to typical Torch style code, with a model, dataset, dataloader, optimizer, and training loop. There are some differences however which we will discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c507b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wslearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38474b33",
   "metadata": {},
   "source": [
    "`wslearn` provides ready-made datasets for use. Examples from `wslearn` datasets have transformations included for use with consistency-regularisation algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f10449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:12<00:00, 13.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "from wslearn.datasets import Cifar10\n",
    "\n",
    "data = Cifar10(num_lbl=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1af47",
   "metadata": {},
   "source": [
    "There are separate labelled and unlabelled datasets. When accessing examples, the output is a dictionary of the original data, it's label and the transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c779d92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'y', 'weak', 'medium', 'strong'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_lbl_dataset()[1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd46d2",
   "metadata": {},
   "source": [
    "The unlabelled observations do not have labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc6a1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'weak', 'medium', 'strong'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_ulbl_dataset()[1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8dd61d",
   "metadata": {},
   "source": [
    "`wslearn` provides an implementation of FixMatch (https://arxiv.org/pdf/2001.07685). There are several parameters we can customise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "586b62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wslearn.algorithms import FixMatch\n",
    "\n",
    "algorithm = FixMatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9138f",
   "metadata": {},
   "source": [
    "We can use the torch implementation of ResNet50, and modify the output layer to 10 classes to match CIFAR10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed6ac5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nhamid/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50')\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c1f19",
   "metadata": {},
   "source": [
    "`wslearn` provides specialised dataloaders for handling labelled and unlabelled batches. The CyclicLoader will reshuffle the labelled and unlabelled data separately once they have been consumed. This means the dataloader will never terminate. Output from the CyclicLoader is a tuple labelled_batch, unlabelled_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a755d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wslearn.utils.data import CyclicLoader\n",
    "\n",
    "lbl_batch_size = 8\n",
    "ulbl_batch_size = 16\n",
    "train_loader = CyclicLoader(data.get_lbl_dataset(), data.get_ulbl_dataset(),\n",
    "                               lbl_batch_size=lbl_batch_size, ulbl_batch_size=ulbl_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bdbed",
   "metadata": {},
   "source": [
    "We can simply use Adam as provided by Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58e2184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "weight_decay = 0.0005\n",
    "\n",
    "# optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, nesterov=nesterov, weight_decay=weight_decay)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78756e0",
   "metadata": {},
   "source": [
    "We now need to write a training function. In a wsl context, we prefer to use training iterations rather than epochs, as the idea of an epoch makes less sense with two datasets in parallel. This training loop is otherwise very conventional with the exception of the main training logic being handed over to `algorithm.forward()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939fdc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_device(d, device):\n",
    "    return {k: v.to(device) if torch.is_tensor(v) else v for k, v in d.items()}\n",
    "\n",
    "def train(model, train_loader, algorithm,  optimizer, num_iters=128,\n",
    "          num_log_iters = 8, device=\"cpu\"):\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    training_bar = tqdm(train_loader, total=num_iters, desc=\"Training\",\n",
    "                        leave=True)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i, (lbl_batch, ulbl_batch) in enumerate(training_bar):\n",
    "\n",
    "        lbl_batch = dict_to_device(lbl_batch, device)\n",
    "        ulbl_batch = dict_to_device(ulbl_batch, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = algorithm.forward(model, lbl_batch, ulbl_batch)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (i+1)\n",
    "\n",
    "        if i % num_log_iters == 0:\n",
    "            training_bar.set_postfix(avg_loss = round(avg_loss, 4))\n",
    "\n",
    "        if i > num_iters:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3addcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2048 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FixMatch' object has no attribute 'forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, algorithm, optimizer, num_iters, num_log_iters, device)\u001b[39m\n\u001b[32m     18\u001b[39m ulbl_batch = dict_to_device(ulbl_batch, device)\n\u001b[32m     20\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m loss = \u001b[43malgorithm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m(model, lbl_batch, ulbl_batch)\n\u001b[32m     24\u001b[39m loss.backward()\n\u001b[32m     26\u001b[39m optimizer.step()\n",
      "\u001b[31mAttributeError\u001b[39m: 'FixMatch' object has no attribute 'forward'"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train(model=model, train_loader=train_loader, algorithm=algorithm,\n",
    "      optimizer=optimizer, device=device, num_iters=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9e0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "def evaluate(model, eval_loader, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_num = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "    y_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "\n",
    "            num_batch = y.shape[0]\n",
    "            total_num += num_batch\n",
    "\n",
    "            logits = model(X)\n",
    "\n",
    "\n",
    "\n",
    "            loss = F.cross_entropy(logits, y, reduction='mean', ignore_index=-1)\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(torch.max(logits, dim=-1)[1].cpu().tolist())\n",
    "            y_logits.append(logits.cpu().numpy())\n",
    "            y_probs.extend(torch.softmax(logits, dim=-1).cpu().tolist())\n",
    "            total_loss += loss.item() * num_batch\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_logits = np.concatenate(y_logits)\n",
    "        top1 = accuracy_score(y_true, y_pred)\n",
    "        # top5 = top_k_accuracy_score(y_true, y_pred, k=5)\n",
    "        balanced_top1 = balanced_accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='macro')\n",
    "        recall = recall_score(y_true, y_pred, average='macro')\n",
    "        F1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        print(\"accuracy: \", top1)\n",
    "        # print(\"accuracy top 5: \", top5)\n",
    "        print(\"balanced-accuracy: \", balanced_top1)\n",
    "        print(\"recall: \", recall)\n",
    "        print(\"f1: \", F1)\n",
    "\n",
    "        cf_mat = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "        print('confusion matrix:\\n' + np.array_str(cf_mat))\n",
    "\n",
    "        model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9162594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.1299\n",
      "balanced-accuracy:  0.1299\n",
      "recall:  0.1299\n",
      "f1:  0.07572577356234492\n",
      "confusion matrix:\n",
      "[[0.    0.003 0.203 0.    0.021 0.    0.481 0.    0.218 0.074]\n",
      " [0.    0.002 0.24  0.008 0.083 0.    0.513 0.    0.124 0.03 ]\n",
      " [0.    0.    0.157 0.001 0.097 0.    0.691 0.    0.035 0.019]\n",
      " [0.    0.001 0.174 0.    0.148 0.    0.65  0.    0.013 0.014]\n",
      " [0.    0.    0.222 0.002 0.088 0.    0.659 0.    0.023 0.006]\n",
      " [0.    0.    0.119 0.001 0.219 0.    0.64  0.    0.013 0.008]\n",
      " [0.    0.001 0.143 0.    0.12  0.    0.729 0.    0.003 0.004]\n",
      " [0.    0.    0.091 0.001 0.135 0.    0.716 0.    0.048 0.009]\n",
      " [0.    0.002 0.242 0.001 0.058 0.    0.383 0.    0.302 0.012]\n",
      " [0.    0.    0.096 0.001 0.041 0.    0.667 0.    0.174 0.021]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhamid/miniconda3/envs/usb/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_loader = DataLoader(data.get_eval_dataset(), batch_size=32)\n",
    "evaluate(model, eval_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4b891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wslearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
