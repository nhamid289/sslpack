{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede7c638",
   "metadata": {},
   "source": [
    "# Transform example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12276e23",
   "metadata": {},
   "source": [
    "In this example we will use a consistency regularisation method with some transformations to achieve semi supervised learning. We are using MNIST with a simple CNN for classification, trained using the FixMatch algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578fe050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wslearn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322afd7b",
   "metadata": {},
   "source": [
    "Importing the data from torchvision and normalising the pixel values to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe53d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "mnist_tr = MNIST(root=\"~/.wslearn/datasets\", train=True, download=True)\n",
    "mnist_ts = MNIST(root=\"~/.wslearn/datasets\", train=False, download=True)\n",
    "\n",
    "X_tr, y_tr = mnist_tr.data.float()/255, mnist_tr.targets\n",
    "X_ts, y_ts = mnist_ts.data.float()/255, mnist_ts.targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a4792",
   "metadata": {},
   "source": [
    "Let's define some transformations. FixMatch expects a weak and strong transformation to be defined. Since the data is currently a torch tensor, we convert it to PIL image then apply any transforms, before converting it back to a torch tensor. The weak transform is just a random horizontal flip, the strong transform is a flip along with a random augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9aadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0659cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lb_ulb_balanced(X, y, num_lbl, num_ulbl = None, return_idx = False,\n",
    "                          seed=None):\n",
    "    \"\"\"\n",
    "    A function to split features and labels into separate labelled and\n",
    "    unlabelled sets.\n",
    "\n",
    "    Args:\n",
    "        X: the features\n",
    "        y: the labels\n",
    "        num_classes: The number of target classes\n",
    "        num_lbl: The number of samples per class to be labelled\n",
    "        num_ulbl: The number of samples per class to be unlabelled.\n",
    "            If left unspecified, all remaining unlabelled data is taken\n",
    "\n",
    "    Returns\n",
    "        If return_idx is True:\n",
    "            Returns a tuple of lists containing the labelled and unlabelled\n",
    "            indices\n",
    "        Else:\n",
    "            Returns a 4-tuple containing the labelled features and labels,\n",
    "            and unlabelled features and labels\n",
    "    \"\"\"\n",
    "\n",
    "    # lbls = [] if lbl_idx is None else lbl_idx\n",
    "    # ulbls = [] if ulbl_idx is None else ulbl_idx\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    lbls = []\n",
    "    ulbls = []\n",
    "\n",
    "    for c in torch.unique(y):\n",
    "        c = c.item()\n",
    "        idx = torch.where(y == c)[0]\n",
    "\n",
    "        shuffled = idx[torch.randperm(len(idx))]\n",
    "\n",
    "        lbls.extend(shuffled[:num_lbl].tolist())\n",
    "        if num_ulbl is None:\n",
    "            # all remaining examples are made unlabelled\n",
    "            ulbls.extend(shuffled[num_lbl:].tolist())\n",
    "        else:\n",
    "            ulbls.extend(shuffled[num_lbl:num_lbl + num_ulbl].tolist())\n",
    "\n",
    "    lbls = torch.tensor(lbls, dtype=torch.long)\n",
    "    ulbls = torch.tensor(ulbls, dtype=torch.long)\n",
    "\n",
    "    if return_idx:\n",
    "        return lbls, ulbls\n",
    "\n",
    "    return X[lbls], y[lbls], X[ulbls], y[ulbls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10919a",
   "metadata": {},
   "source": [
    "We are using a CNN so we will add a channels dimension (only one channel in this case for graycsale images). Then we can split the data into the labelled and unlabelled parts, and use TransformDataset from wslearn to obtain samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bbdc66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wslearn.utils.data import TransformDataset\n",
    "from wslearn.utils.data import split_lb_ulb_balanced\n",
    "\n",
    "X_tr = X_tr.unsqueeze(1)\n",
    "X_ts = X_ts.unsqueeze(1)\n",
    "num_labels_per_class = 4\n",
    "\n",
    "\n",
    "X_tr_lb, y_tr_lb, X_tr_ulb, y_tr_ulb = split_lb_ulb_balanced(X_tr, y_tr, num_labels_per_class)\n",
    "\n",
    "\n",
    "lbl_dataset = TransformDataset(X_tr_lb, y_tr_lb, weak_transform=weak_transform, strong_transform=strong_transform)\n",
    "ulbl_dataset = TransformDataset(X_tr_ulb, y_tr_ulb, weak_transform=weak_transform, strong_transform=strong_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3877cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wslearn.algorithms import FixMatch\n",
    "\n",
    "algorithm = FixMatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b69f1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from wslearn.algorithms import Algorithm\n",
    "from wslearn.utils.criterions import ce_consistency_loss\n",
    "\n",
    "class FixMatch(Algorithm):\n",
    "    \"\"\" An implementation of FixMatch (https://arxiv.org/pdf/2001.07685)\n",
    "\n",
    "    By default the algorithm uses cross entropy loss for the supervised part,\n",
    "    and cross entropy consistency loss for the unsupervised part.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambda_u=0.5,  conf_threshold=0.95,\n",
    "                 max_pseudo_labels = None, sup_loss_func=None,\n",
    "                 unsup_loss_func=None):\n",
    "        \"\"\"\n",
    "        Initialise a fixmatch algorithm.\n",
    "\n",
    "        Args:\n",
    "            use_hard_label: true if using hard labelling for pseudo labels,\n",
    "                otherwise soft labelling is used\n",
    "            lambda_u: the weight of unlabelled loss in the total loss\n",
    "            conf_threshold: the confidence threshold for pseudo-labels\n",
    "            sup_loss_func: a function with signature f(pred, true) to compute\n",
    "                the loss on the supervised batch\n",
    "            unsup_loss_func: a function with signature f(pred, true, mask) to\n",
    "                compute the loss on the unsupervised batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.lambda_u = lambda_u\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.max_pseudo_labels = max_pseudo_labels\n",
    "\n",
    "        if sup_loss_func is None:\n",
    "            # Default reduction is 'mean'\n",
    "            self.sup_loss_func = cross_entropy\n",
    "        else:\n",
    "            self.sup_loss_func = sup_loss_func\n",
    "\n",
    "        if unsup_loss_func is None:\n",
    "            # Default reduction is 'mean'\n",
    "            self.unsup_loss_func = ce_consistency_loss\n",
    "        else:\n",
    "            self.unsup_loss_func = unsup_loss_func\n",
    "\n",
    "    def forward(self, model, lbl_batch, ulbl_batch, log_func=None):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of FixMatch\n",
    "\n",
    "        Args:\n",
    "            model: The predictor model\n",
    "            lbl_batch: A dictionary with labelled data using keys \"X\", \"y\"\n",
    "            ubl_batch: A dictionary with unlabelled data using keys \"X\", \"y\"\n",
    "            log_func: A function which accepts a dictionary containing some\n",
    "                training information\n",
    "        \"\"\"\n",
    "\n",
    "        x_lbl_weak = lbl_batch[\"weak\"]\n",
    "        x_ulbl_weak = ulbl_batch[\"weak\"]\n",
    "        x_ulbl_strong = ulbl_batch[\"strong\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_ulbl_weak)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            confidences, pseudo_labels = torch.max(probs, dim=1)\n",
    "            mask = confidences.ge(self.conf_threshold)\n",
    "            if self.max_pseudo_labels is not None:\n",
    "                _, indices = torch.topk(confidences,\n",
    "                                        min(self.max_pseudo_labels,\n",
    "                                            len(confidences)))\n",
    "                keep = torch.zeros_like(mask, dtype=torch.bool)\n",
    "                keep[indices] = True\n",
    "                mask &= keep\n",
    "\n",
    "        x = torch.concat([x_lbl_weak, x_ulbl_strong])\n",
    "        out = model(x)\n",
    "        out_lbl_weak = out[:x_lbl_weak.size(0)]\n",
    "        out_ulbl_strong = out[x_lbl_weak.size(0):]\n",
    "\n",
    "        sup_loss = self.sup_loss_func(out_lbl_weak, lbl_batch[\"y\"])\n",
    "\n",
    "        unsup_loss = self.unsup_loss_func(out_ulbl_strong, pseudo_labels, mask)\n",
    "\n",
    "        total_loss = (1 - self.lambda_u) * sup_loss + self.lambda_u * unsup_loss\n",
    "\n",
    "        if log_func is not None:\n",
    "            log_func({\n",
    "                \"sup_loss\": sup_loss,\n",
    "                \"unsup_loss\": unsup_loss,\n",
    "                \"total_loss\": total_loss,\n",
    "                \"mask\": mask,\n",
    "                \"pseudo_labels\": pseudo_labels\n",
    "            })\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    algorithm = FixMatch(lambda_u=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39417cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wslearn.utils.data import CyclicLoader\n",
    "\n",
    "lbl_batch_size = 20\n",
    "ulbl_batch_size = 60\n",
    "train_loader = CyclicLoader(lbl_dataset, ulbl_dataset, lbl_batch_size=lbl_batch_size, ulbl_batch_size=ulbl_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbdec952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_device(d, device):\n",
    "    return {k: v.to(device) if torch.is_tensor(v) else v for k, v in d.items()}\n",
    "\n",
    "def train(model, train_loader, algorithm,  optimizer, num_iters=128,\n",
    "          num_log_iters = 8, device=\"cpu\"):\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    training_bar = tqdm(train_loader, total=num_iters, desc=\"Training\",\n",
    "                        leave=True)\n",
    "\n",
    "    for i, (lbl_batch, ulbl_batch) in enumerate(training_bar):\n",
    "\n",
    "        lbl_batch = dict_to_device(lbl_batch, device)\n",
    "        ulbl_batch = dict_to_device(ulbl_batch, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = algorithm.forward(model, lbl_batch, ulbl_batch)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % num_log_iters == 0:\n",
    "            training_bar.set_postfix(loss = round(loss.item(), 4))\n",
    "\n",
    "        if i > num_iters:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "773ae93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 max pooling\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)  # 28 -> 14 -> 7 after 2 poolings\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> [batch, 16, 14, 14]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> [batch, 32, 7, 7]\n",
    "        x = x.view(x.size(0), -1)             # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca6c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed725530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, algorithm, optimizer, num_iters, num_log_iters, device)\u001b[39m\n\u001b[32m      9\u001b[39m model.train()\n\u001b[32m     11\u001b[39m training_bar = tqdm(train_loader, total=num_iters, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m                     leave=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlbl_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mulbl_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlbl_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlbl_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mulbl_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mulbl_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/wslearn/utils/data/dataloader.py:79\u001b[39m, in \u001b[36mCyclicLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m         lbl_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlbl_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28mself\u001b[39m.lbl_iter = \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m.lbl_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/wslearn/utils/data/dataset.py:108\u001b[39m, in \u001b[36mTransformDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    106\u001b[39m     out_dict[\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m] = y\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weak_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     out_dict[\u001b[33m\"\u001b[39m\u001b[33mweak\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweak_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.medium_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    110\u001b[39m     out_dict[\u001b[33m\"\u001b[39m\u001b[33mmedium\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.medium_transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wslearn/lib/python3.11/site-packages/torchvision/transforms/functional.py:142\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    140\u001b[39m     _log_api_usage_once(to_tensor)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil._is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "train(model=model, train_loader=train_loader, algorithm=algorithm,\n",
    "      optimizer=optimizer, device=device, num_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea26318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wslearn.utils.data import BasicDataset\n",
    "\n",
    "X_ts, y_ts = X_ts.float(), y_ts.float()\n",
    "\n",
    "test_dataset = BasicDataset(X_ts, y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix\n",
    ")\n",
    "\n",
    "def evaluate(model, eval_loader, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_num = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "            num_batch = y.shape[0]\n",
    "            total_num += num_batch\n",
    "            logits = model(X)\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(torch.max(logits, dim=-1)[1].cpu().tolist())\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        print(\"accuracy: \", acc)\n",
    "        cf_mat = confusion_matrix(y_true, y_pred)\n",
    "        np.round(cf_mat, 2)\n",
    "        print('confusion matrix:\\n' + np.array_str(cf_mat))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9db37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7617\n",
      "confusion matrix:\n",
      "[[ 931    2    3    2    0   18   10    5    9    0]\n",
      " [   1 1118    8    0    1    1    2    3    1    0]\n",
      " [  10    1  697  206    3   83   10   17    5    0]\n",
      " [   0    0   12  934    0   37    2   18    6    1]\n",
      " [   4    1    3    0  959    1    9    4    0    1]\n",
      " [   2    0    3  563    0  277    3   36    7    1]\n",
      " [  36    6   39   18    9   16  819    1   14    0]\n",
      " [   0    4    7   11    7   25    1  966    1    6]\n",
      " [  16   13    4  172    5   64    2   93  605    0]\n",
      " [  14    8    3   35   68    4    0  522   44  311]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd0e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ff0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wslearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
